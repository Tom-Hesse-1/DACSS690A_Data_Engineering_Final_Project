# DACSS690A_Data_Engineering_Final_Project
Files for the Final Project for DACSS690A Data Engineering, Fall 2024

**Introduction**
This is my final project for DACSS690A-01 Data Engineering, Fall 2024 Semester, at the University of Massachusetts Amherst. This project is a working Extract, Transform, Load (ETL) pipeline. It extracts data from the Arizona Department of Transporation's (ADOT) Application Programming Interface (API) using Python's Requests library. The data is then transformed using Python language, including various packages such as SQLite3 and uuid. 
  The primary data transformation steps included the changing of some of the API data's columns' data types to types that are supported by SQLite, and the creation of new tables designed for easy analysis by less technically-skilled individuals. The original data from the API included 5 tables: 1) Cameras, 2) Weather Stations, 3) Message Boards, 4) Events, and 5) Alerts, with each table containing numerous columns. Numerous columns across the tables were transformed from unsupported data types (e.g. boolean, lists, etc.) to supported data types. The Alerts table is empty of all data, so it was not included in the Transformation & Loading steps of the project. Duplicate rows in the API data were also removed to improve the data's clarity and concision. Two new additional tables were created: "Unique_Messages" and "Unique_Event_Types". Unique_Messages contains all unique messages being displayed on message boards across the State of Arizona by ADOT, a unique identifier for each message, and the count of how many times that message is displayed. It is designed to quickly give interested parties insight into the state of transportation on Arizona's roadways by showing which of ADOT's messages are most prevalent. "Unique_Event_Types" contains all unique types of transportation-related events in Arizona, along with unique identifiers for each event type, and the count of how many of each event types are currently in existence. This table also gives quick, meaningful insight into the state of Transportation in Arizona by easily showing what types of events are most common in the state. Interested parties can use the data in these new tables to inform decision making in how best to monitor and operate transportation in Arizona. For example, if collisions are especially prevalent at a given point in time, the appropriate agencies can dispatch additional first responder personnel to help handle the increased number of automobile accidents.
  The data is loaded into a SQLite database that is created and populated when the python file in this repository (de_etl_pipeline_final.py) is ran. The data is found across 6 tables. 4 of the SQLite tables are roughly equivalent to their corresponding tables in the API data, with the remaining 2 tables being the additional ones I designed, described in the paragraph above. The original tables can be easily queried once the python document is ran. These tables can be queried and manipulated in myriad ways by the user with fresh data from the ADOT API for many analytical purposes.
  Automation is handled via Prefect. I broke up nearly the entire python script into prefect tasks and flows that prefect will deploy on a one-hour interval schedule, as long as the server is running. All portions of the Extract, Transform, and Load pipeline are programmed to execute on this schedule. This means that fresh data from the ADOT API will be extracted, transformed, and loaded into the SQLite databases, ready to be queried and analyzed, every hour. The flows, tasks, and initial deployment were implemented programmatically via Python script in the Python file, and scheduling was implemented via the Prefect UI (see screenshot in repository). Scheduled deployments are set to cease their 1-hour interval iterations at 12:00:00AM EST on 12/19/2024, the end of the Fall 2024 Semester. Future, scheduled, automatic deployments can easily be resumed via Prefect.


**Instructions**
  1) Download file "de_etl_pipeline_final.py"
  2) From Command Prompt, launch file listed in step 1 from it's directory
  3) From Command Prompt, launch Prefect UI
  4) Data has then been extracted from ADOT API, Transformed for easy querying and analysis, and loaded into a SQLite database. Workflow & Automation can be observed adjusted from Prefect UI.
